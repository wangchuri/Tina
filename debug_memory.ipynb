{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=r\"D:\\wangchuri\\development\\project\\tina\\tina-sauce----tcgai\\model\\GGUF\\qwen2.5-7b-instruct-q4_k_m.gguf\",\n",
    "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"是的搜索一下qwen\"\n",
    "from tina.Tools.tools import Tools\n",
    "tools = Tools()\n",
    "tools.register(\n",
    "\tname = \"query\",\n",
    " \tdescription = \"搜索引擎查询\",\n",
    "    required_parameters = [\"query\"],\n",
    "\tparameters = {\n",
    "\t\t\"query\": {\n",
    "\t\t\t\"type\": \"string\",\n",
    "\t\t\t\"description\": \"搜索内容\"\n",
    "\t\t},\n",
    "        \"n\":{\n",
    "\t\t\t\"type\": \"integer\",\n",
    "\t\t\t\"description\": \"搜索结果的数量\"\n",
    "\t\t}\n",
    "        }\n",
    ")\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    # max_tokens=200,\n",
    "    stream=False,\n",
    "    tools = tools.tools\n",
    "    # tools = [\n",
    "\t#     {\n",
    "\t# \t\"type\": \"function\", # 约定的字段 type，目前支持 function 作为值\n",
    "\t# \t\"function\": { # 当 type 为 function 时，使用 function 字段定义具体的函数内容\n",
    "\t# \t\t\"name\": \"search\", # 函数的名称，请使用英文大小写字母、数据加上减号和下划线作为函数名称\n",
    "\t# \t\t\"description\": \"\"\" \n",
    "\t# \t\t\t通过搜索引擎搜索互联网上的内容。\n",
    " \n",
    "\t# \t\t\t当你的知识无法回答用户提出的问题，或用户请求你进行联网搜索时，调用此工具。请从与用户的对话中提取用户想要搜索的内容作为 query 参数的值。\n",
    "\t# \t\t\t搜索结果包含网站的标题、网站的地址（URL）以及网站简介。\n",
    "\t# \t\t\"\"\", # 函数的介绍，在这里写上函数的具体作用以及使用场景，以便 Kimi 大模型能正确地选择使用哪些函数\n",
    "\t# \t\t\"parameters\": { # 使用 parameters 字段来定义函数接收的参数\n",
    "\t# \t\t\t\"type\": \"object\", # 固定使用 type: object 来使 Kimi 大模型生成一个 JSON Object 参数\n",
    "\t# \t\t\t\"required\": [\"query\"], # 使用 required 字段告诉 Kimi 大模型哪些参数是必填项\n",
    "\t# \t\t\t\"properties\": { # properties 中是具体的参数定义，你可以定义多个参数\n",
    "\t# \t\t\t\t\"query\": { # 在这里，key 是参数名称，value 是参数的具体定义\n",
    "\t# \t\t\t\t\t\"type\": \"string\", # 使用 type 定义参数类型\n",
    "\t# \t\t\t\t\t\"description\": \"\"\"\n",
    "\t# \t\t\t\t\t\t用户搜索的内容，请从用户的提问或聊天上下文中提取。\n",
    "\t# \t\t\t\t\t\"\"\" # 使用 description 描述参数以便 Kimi 大模型更好地生成参数\n",
    "\t# \t\t\t\t}\n",
    "\t# \t\t\t}\n",
    "\t# \t\t}\n",
    "\t# \t}\n",
    "\t# }\n",
    "    # ]\n",
    ")\n",
    "# messages = ''\n",
    "# for chunk in output:\n",
    "#     delta = chunk['choices'][0]['delta']\n",
    "#     if 'role' in delta:\n",
    "#         messages += delta['role']\n",
    "#     elif 'content' in delta:\n",
    "#         messages += delta['content']\n",
    "# output_list = list(output)\n",
    "# print(messages)\n",
    "print(output['choices'][0]['message'])\n",
    "messages = output['choices'][0]['message']\n",
    "import json\n",
    "# json_data = json.dumps(messages, ensure_ascii=False, indent=4)\n",
    "# print(json_data)\n",
    "content = messages['content'].replace('\\n', '')\n",
    "print(content[12:-13])\n",
    "main_message = content[12:-13]\n",
    "de_message = content[0:11] + content[-12:]\n",
    "print(de_message)\n",
    "json_data = json.dumps(main_message, ensure_ascii=False, indent=4)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# json_data = json.dumps(messages, ensure_ascii=False, indent=4)\n",
    "# print(json_data)\n",
    "content = messages['content'].replace('\\n', '')\n",
    "print(content[12:-13])\n",
    "main_message = content[12:-13]\n",
    "de_message = content[0:11] + content[-12:]\n",
    "print(de_message)\n",
    "json_data = json.loads(main_message)\n",
    "print(json_data)\n",
    "print(json_data['name'])\n",
    "print(json_data['arguments']['n'])\n",
    "print(isinstance(messages, dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    \"请尝试搜索一下langchain，将查询到的结果总结一份给我\",\n",
    "    max_tokens = 1000,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"人工智能的未来发展方向是什么？\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        print(delta['role'], end=': ', flush=True)\n",
    "    elif 'content' in delta:\n",
    "        print(delta['content'], end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
